# Gupy Jobs ETL Pipeline ğŸ“¡

## ğŸ“Œ Overview
This project is an **ELT (Extract, Load, Transform) pipeline** that processes *job listing data* from Gupy, categorizes it, and loads it into **Google BigQuery** for analysis. The goal is to enable **efficient job data analysis and visualization** using a BI tool such as Looker Studio.

## ğŸ—ï¸ Architecture
1. **Extract**: Fetches job listings from the Gupy URL.
2. **Transform**: Cleans and enriches the data by mapping job categories, states, and vacancy types.
3. **Load**: Stores the processed data in **Google BigQuery** for further analysis.

## ğŸ› ï¸ Tech Stack
- **Python** ğŸ - Main programming language ([Python](https://www.python.org/))
- **Google Cloud Storage (GCS)** â˜ï¸ - Stores raw job data ([GCS](https://cloud.google.com/storage))
- **BigQuery** ğŸ“Š - Data warehouse for structured storage ([BigQuery](https://cloud.google.com/bigquery))
- **Apache Airflow (Astro)** ğŸŒ¬ï¸ - Orchestrates the ELT workflow ([Astronomer](https://www.astronomer.io/))
- **Looker Studio** ğŸ“ˆ - Data visualization and dashboarding ([Looker Studio](https://lookerstudio.google.com/))


## ğŸ“œ SQL Transformations
The **BigQuery SQL script** performs the following transformations:
- Maps **state names** to abbreviations (e.g., `SÃ£o Paulo` â†’ `SP`)
- Categorizes **job positions** (e.g., `Desenvolvedor` â†’ `TI & Desenvolvimento`)
- Maps **vacancy types** (e.g., `vacancy_type_remote` â†’ `Remoto`)
- Cleans and loads data into a medalion structured base
    1. Bronze ğŸŸ¤ â€“ Raw data, ingested as-is from the source.
    2. Silver âšª â€“ Cleaned and structured data, often with some transformations.
    3. Gold ğŸŸ¡ â€“ Curated, aggregated, and ready-for-analysis data

## ğŸš€ How to Run the Pipeline
1. **Setup Google Cloud**:
   - Create a **GCS bucket** for storage.
   - Set up a **BigQuery dataset**.

2. **Create your GCP connection following Astronomer Doc**
    - [Connection doc for BQ](https://www.astronomer.io/docs/learn/connections/bigquery/?tab=key-file-value#bigquery-connection)

4. **Orchestrate with Airflow**:
   - Start Airflow:
     ```bash
     astro dev start
     ```
   - Trigger the DAG manually or schedule it.
--------
> Ps.: I strongly recommend reading the following for a better understanding of the whole process.
> 1. ğŸ§®[BigQuery Doc](https://cloud.google.com/bigquery/docs?authuser=1)
> 2. â˜ï¸ğŸ“¦[CloudStorage Doc](https://cloud.google.com/storage/docs?authuser=1)
> 3. ğŸ’«[Astronomer Doc](https://www.astronomer.io/docs/) 

## ğŸ“Š Dashboard
After processing, the data is available in **BigQuery** and can be visualized using **Looker Studio**. The dashboard includes:
- Job distribution by category
- Remote vs. onsite job trends

[Dashboard]()

## ğŸ¤ Contributing
Feel free to submit pull requests or suggest improvements! ğŸ˜Š

## ğŸ“„ License
This project is open-source under the **MIT License**.

---
ğŸ’¡ *Happy coding!* ğŸ˜ƒ


## Author
[@ht-yarll](https://github.com/ht-yarll) *AKA me!*

